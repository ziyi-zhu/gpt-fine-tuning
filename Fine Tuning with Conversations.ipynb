{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757dcc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Laptop GPU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "# wandb.init(project='gpt-neo-dialogs', entity='ziyizhu')\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436a992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'datasets/train_dataset.csv'\n",
    "test_path = 'datasets/test_dataset.csv'\n",
    "\n",
    "model_path = \"./models/gpt-neo-dialogs\"\n",
    "test_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567862c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv('./datasets/raw/cornell-movie-dialogs-corpus/movie_lines.txt',\n",
    "                    sep=re.escape(' +++$+++ '),\n",
    "                    names=['lineID', 'characterID', 'movieID', 'characterName', 'utterance'],\n",
    "                    index_col=0,\n",
    "                    engine='python', \n",
    "                    encoding=\"latin1\")\n",
    "\n",
    "conversations = pd.read_csv('./datasets/raw/cornell-movie-dialogs-corpus/movie_conversations.txt',\n",
    "                    sep=re.escape(' +++$+++ '),\n",
    "                    names=['characterID_0', 'characterID_1', 'movieID', 'utteranceList'],\n",
    "                    engine='python', \n",
    "                    encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8d2f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11ad036e03a44368a7e62d7579210bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 74787\n",
      "Test dataset length: 8310\n"
     ]
    }
   ],
   "source": [
    "def generate_datasets(conversations, lines):\n",
    "    dataset = []\n",
    "    for index, row in tqdm(conversations.iterrows(), total=conversations.shape[0]):\n",
    "        data = []\n",
    "        for line_id in eval(row['utteranceList']):\n",
    "            line = lines.loc[line_id]\n",
    "            data.append(f'{str(line.characterName).title()} said: \"{re.sub(\" +\", \" \", str(line.utterance))}\"')\n",
    "        dataset.append('\\n'.join(data))\n",
    "\n",
    "    dataset = pd.DataFrame(dataset, columns=['text'])\n",
    "    return train_test_split(dataset, test_size=test_size, shuffle=False)\n",
    "\n",
    "train, test = generate_datasets(conversations, lines)\n",
    "\n",
    "print(\"Train dataset length: \" + str(len(train)))\n",
    "print(\"Test dataset length: \" + str(len(test)))\n",
    "\n",
    "train.to_csv(train_path, index=False)\n",
    "test.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef0aba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bianca said: \"Can we make this quick?  Roxanne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bianca said: \"You're asking me out.  That's so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bianca said: \"No, no, it's my fault -- we didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cameron said: \"Why?\"\\nBianca said: \"Unsolved m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bianca said: \"Gosh, if only we could find Kat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Bianca said: \"Can we make this quick?  Roxanne...\n",
       "1  Bianca said: \"You're asking me out.  That's so...\n",
       "2  Bianca said: \"No, no, it's my fault -- we didn...\n",
       "3  Cameron said: \"Why?\"\\nBianca said: \"Unsolved m...\n",
       "4  Bianca said: \"Gosh, if only we could find Kat ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663ad92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77907704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kabuo said: \"What makes you think I play?\"\n",
      "Nels said: \"If I want to play, I'll play.\"\n",
      "Nigerian man: \"That's the reason to play. You have been there before. If you want you, I'll play.\"\n",
      "Nigerian man: \"That's what I'll do.\"\n",
      "Nigerian man: \"You're probably right, Nels. You're probably right.\"\n",
      "Nigerian man: \"That\n",
      "Kabuo said: \"What makes you think I play?\"\n",
      "Nels said: \"The world is over, the world is over. What makes you think I play?\"\n",
      "\"That's funny,\" Maki said. \"I didn't see those two players playing.\"\n",
      "They shook hands as the pair of the girls headed for the door, and they both stood up. Jiro and Kaba were the only players remaining after their final tournament, and they were already engaged in a game\n",
      "Kabuo said: \"What makes you think I play?\"\n",
      "Nels said: \"That's a good point. So you know what it's like to play.\" I should have said that. You don't have to think about it. I am sure you have a great deal of respect for the players who we had in your first game. I think the players who played in my first game were really good, and they had a lot of respect. To the players who played at the\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Kabuo said: \"What makes you think I play?\"\n",
    "Nels said: \"'''\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100, num_return_sequences=3)\n",
    "result = tokenizer.batch_decode(generated_tokens)\n",
    "\n",
    "for text in result:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acfc81d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8ec15e1b83c807f7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\zhuzi\\.cache\\huggingface\\datasets\\csv\\default-8ec15e1b83c807f7\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce58ee744c435c9c4c9de6a00fce0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885696622c2b43ec9b9108b228951905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\zhuzi\\.cache\\huggingface\\datasets\\csv\\default-8ec15e1b83c807f7\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec4071cfaff462c9d6bca9a2b458491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8d841a80914317a0fbc59f8f3894be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaa4cf26c7e4f8ab1d54637057a9a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "datasets = load_dataset('csv', data_files={'train': train_path, 'test': test_path})\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93bf6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c508683c687a46e6a8c56941128ec93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6398d3c2de54b4494ae43308b6c37b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c751f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,         # The output directory\n",
    "    overwrite_output_dir=True,     # overwrite the content of the output directory\n",
    "    num_train_epochs=3,            # number of training epochs\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type='linear',\n",
    "    adafactor=False,\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    per_device_eval_batch_size=16, # batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,                # Number of update steps between two evaluations.\n",
    "    save_steps=1000,               # after # steps model is saved\n",
    "    warmup_steps=500,              # number of warmup steps for learning rate scheduler\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3637229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 51:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.922000</td>\n",
       "      <td>2.837582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.763300</td>\n",
       "      <td>2.805496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.755300</td>\n",
       "      <td>2.795511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.746300</td>\n",
       "      <td>2.788532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.733100</td>\n",
       "      <td>2.784108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.717200</td>\n",
       "      <td>2.780734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.715300</td>\n",
       "      <td>2.778864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.710100</td>\n",
       "      <td>2.776872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.698000</td>\n",
       "      <td>2.774550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.698600</td>\n",
       "      <td>2.773267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.700500</td>\n",
       "      <td>2.771392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.660900</td>\n",
       "      <td>2.771598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.676500</td>\n",
       "      <td>2.770983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.675600</td>\n",
       "      <td>2.770476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.670300</td>\n",
       "      <td>2.769443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.670900</td>\n",
       "      <td>2.769507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.658800</td>\n",
       "      <td>2.768887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.666600</td>\n",
       "      <td>2.768210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.654300</td>\n",
       "      <td>2.767757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.656800</td>\n",
       "      <td>2.767274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.654400</td>\n",
       "      <td>2.767057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.658500</td>\n",
       "      <td>2.767110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.650700</td>\n",
       "      <td>2.767556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.642300</td>\n",
       "      <td>2.767028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.623300</td>\n",
       "      <td>2.767251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.647800</td>\n",
       "      <td>2.766994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.634300</td>\n",
       "      <td>2.767059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.641000</td>\n",
       "      <td>2.766938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.651300</td>\n",
       "      <td>2.766702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.641800</td>\n",
       "      <td>2.766588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.641900</td>\n",
       "      <td>2.766356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.637800</td>\n",
       "      <td>2.766402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16413, training_loss=2.682661901246344, metrics={'train_runtime': 3111.3796, 'train_samples_per_second': 42.201, 'train_steps_per_second': 5.275, 'total_flos': 8574384177414144.0, 'train_loss': 2.682661901246344, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d956af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/gpt-neo-dialogs\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53fd2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.config.test_size = test_size\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3022cd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models/gpt-neo-dialogs\\config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading configuration file ./models/gpt-neo-dialogs\\config.json\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file ./models/gpt-neo-dialogs\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at ./models/gpt-neo-dialogs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kabuo said: \"What makes you think I play?\"\n",
      "Nels said: \"You have no idea.\"Nels said: \"They're not allowed in school.\"\n",
      "Kabuo said: \"You can stay in the house till you die.\"\n",
      "Nels said: \"Is there a school in Tofino?\"Kabuo said: \"I don't know.  I just have to play this game.\"\n",
      "Nels said: \"What is that, Keb\n",
      "Kabuo said: \"What makes you think I play?\"\n",
      "Nels said: \"I'll play, if you like.\"\n",
      "Von Kremen said: \"I play, I am a part of the game.  A little bit of the game...it's an adventure.\"\n",
      "Nels said: \"What games?                               \n",
      "Kabuo said: \"What makes you think I play?\"\n",
      "Nels said: \"I'm not trying to help you, Mama.\"\n",
      "Mumford said: \"I am an adult. But I don't get married.\"\n",
      "Nels said: \"And you don't get any kids.\"Nels said: \"You don't have any kids.\"\n",
      "Mumford said: \"So... it's for a girl who's already pregnant.\"\n",
      "Nels said: \"It\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model_path, tokenizer='EleutherAI/gpt-neo-125M')\n",
    "result = generator(prompt, do_sample=True, temperature=0.9, max_length=100, num_return_sequences=3)\n",
    "\n",
    "for text in result:\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97564d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
