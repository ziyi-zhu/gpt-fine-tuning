{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757dcc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Laptop GPU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436a992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'datasets/train_dataset.csv'\n",
    "test_path = 'datasets/test_dataset.csv'\n",
    "\n",
    "model_path = \"./models/gpt-neo-dialogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfc81d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d27f0e2a7b0ecb18\n",
      "Reusing dataset csv (C:\\Users\\zhuzi\\.cache\\huggingface\\datasets\\csv\\default-d27f0e2a7b0ecb18\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2dc65bc1f44eb98d069c7492326535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddef21fd4b3c4fda88f7b250dae7d444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329f1c68430d40119a5a7866887030d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "datasets = load_dataset('csv', data_files={'train': train_path, 'test': test_path})\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93bf6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8f47a47466428b8e1b730e3e0d6d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafc54b349f947198cf0b7ddcb10093e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0730d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">logical-resonance-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/pnmq01vj\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/pnmq01vj</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_004705-pnmq01vj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 54:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.826800</td>\n",
       "      <td>2.844819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.758500</td>\n",
       "      <td>2.845232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.761100</td>\n",
       "      <td>2.840645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.749400</td>\n",
       "      <td>2.840839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.735000</td>\n",
       "      <td>2.833065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.713400</td>\n",
       "      <td>2.833573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.709500</td>\n",
       "      <td>2.827738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.701600</td>\n",
       "      <td>2.825691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.686700</td>\n",
       "      <td>2.821231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.684800</td>\n",
       "      <td>2.818866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.676000</td>\n",
       "      <td>2.819370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.531300</td>\n",
       "      <td>2.827535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.547300</td>\n",
       "      <td>2.825593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.547800</td>\n",
       "      <td>2.826134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.545900</td>\n",
       "      <td>2.826426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.546700</td>\n",
       "      <td>2.823798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.538300</td>\n",
       "      <td>2.823445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.542500</td>\n",
       "      <td>2.822897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.530700</td>\n",
       "      <td>2.821791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.529300</td>\n",
       "      <td>2.820426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.530700</td>\n",
       "      <td>2.818932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.520100</td>\n",
       "      <td>2.828455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.425100</td>\n",
       "      <td>2.838233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.422800</td>\n",
       "      <td>2.834042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.406300</td>\n",
       "      <td>2.836578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.428000</td>\n",
       "      <td>2.833222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.418400</td>\n",
       "      <td>2.832905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.421200</td>\n",
       "      <td>2.832509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.432200</td>\n",
       "      <td>2.831883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.421700</td>\n",
       "      <td>2.830920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.422100</td>\n",
       "      <td>2.830161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.418000</td>\n",
       "      <td>2.829819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11792<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_004705-pnmq01vj\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_004705-pnmq01vj\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>2.82982</td></tr><tr><td>eval/runtime</td><td>22.5599</td></tr><tr><td>eval/samples_per_second</td><td>220.612</td></tr><tr><td>eval/steps_per_second</td><td>13.83</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>16413</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.418</td></tr><tr><td>train/total_flos</td><td>8574384177414144.0</td></tr><tr><td>train/train_loss</td><td>2.56298</td></tr><tr><td>train/train_runtime</td><td>3268.6562</td></tr><tr><td>train/train_samples_per_second</td><td>40.171</td></tr><tr><td>train/train_steps_per_second</td><td>5.021</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>██▇▇▅▅▃▃▂▁▁▃▃▃▃▂▂▂▂▁▁▄▆▅▆▅▅▅▄▄▄▄</td></tr><tr><td>eval/runtime</td><td>▅▃█▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▄▆▁██▇██████████████████████████</td></tr><tr><td>eval/steps_per_second</td><td>▄▆▁██▇██████████████████████████</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▇▆▆▆▆▆▆▅▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">logical-resonance-8</strong>: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/pnmq01vj\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/pnmq01vj</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cosmic-silence-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/diwzu2qb\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/diwzu2qb</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_014147-diwzu2qb</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 51:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.882500</td>\n",
       "      <td>2.819076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.744800</td>\n",
       "      <td>2.797790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.739900</td>\n",
       "      <td>2.790134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.730100</td>\n",
       "      <td>2.784620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.717500</td>\n",
       "      <td>2.780637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.700300</td>\n",
       "      <td>2.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.698400</td>\n",
       "      <td>2.777615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.692800</td>\n",
       "      <td>2.775136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.680400</td>\n",
       "      <td>2.772970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.680700</td>\n",
       "      <td>2.771894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.680800</td>\n",
       "      <td>2.769991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.622500</td>\n",
       "      <td>2.771959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.637800</td>\n",
       "      <td>2.771858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.636900</td>\n",
       "      <td>2.771525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.632100</td>\n",
       "      <td>2.770394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.633000</td>\n",
       "      <td>2.770167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.621600</td>\n",
       "      <td>2.769513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.628900</td>\n",
       "      <td>2.768780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.617000</td>\n",
       "      <td>2.768166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.618800</td>\n",
       "      <td>2.767565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.617400</td>\n",
       "      <td>2.767475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.619600</td>\n",
       "      <td>2.767843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.600500</td>\n",
       "      <td>2.768840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.593100</td>\n",
       "      <td>2.768208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.574200</td>\n",
       "      <td>2.768579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.598200</td>\n",
       "      <td>2.768144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.585800</td>\n",
       "      <td>2.768359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.592200</td>\n",
       "      <td>2.768274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.602700</td>\n",
       "      <td>2.768053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.593200</td>\n",
       "      <td>2.768004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.593500</td>\n",
       "      <td>2.767886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.589900</td>\n",
       "      <td>2.767869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26052<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_014147-diwzu2qb\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_014147-diwzu2qb\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>2.76787</td></tr><tr><td>eval/runtime</td><td>22.5731</td></tr><tr><td>eval/samples_per_second</td><td>220.484</td></tr><tr><td>eval/steps_per_second</td><td>13.822</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>16413</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5899</td></tr><tr><td>train/total_flos</td><td>8574384177414144.0</td></tr><tr><td>train/train_loss</td><td>2.64739</td></tr><tr><td>train/train_runtime</td><td>3103.8222</td></tr><tr><td>train/train_samples_per_second</td><td>42.304</td></tr><tr><td>train/train_steps_per_second</td><td>5.288</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>█▅▄▃▃▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▃▃▃▂▂▁▁▃▃▃▃▁▂▂▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▆▆▇▇██▆▆▆▆█▇▇▆▆▆▆█████████████</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▆▆▇▇██▆▆▆▆█▇▇▆▆▆▆█████████████</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cosmic-silence-9</strong>: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/diwzu2qb\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/diwzu2qb</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">royal-jazz-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/na5d8ndn\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/na5d8ndn</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_023345-na5d8ndn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 51:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.825900</td>\n",
       "      <td>2.818169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.750700</td>\n",
       "      <td>2.801069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.749900</td>\n",
       "      <td>2.793846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.742500</td>\n",
       "      <td>2.787749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>2.783694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.714200</td>\n",
       "      <td>2.780505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.712300</td>\n",
       "      <td>2.778916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.707000</td>\n",
       "      <td>2.776764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.694500</td>\n",
       "      <td>2.774407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.694900</td>\n",
       "      <td>2.773157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.696100</td>\n",
       "      <td>2.771258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.652700</td>\n",
       "      <td>2.772027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.667900</td>\n",
       "      <td>2.771637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.666800</td>\n",
       "      <td>2.771084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.661300</td>\n",
       "      <td>2.770077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.661500</td>\n",
       "      <td>2.770023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.649300</td>\n",
       "      <td>2.769370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.656700</td>\n",
       "      <td>2.768711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.644000</td>\n",
       "      <td>2.768354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.645900</td>\n",
       "      <td>2.767625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.643200</td>\n",
       "      <td>2.767832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.645600</td>\n",
       "      <td>2.768574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.627300</td>\n",
       "      <td>2.769817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.619300</td>\n",
       "      <td>2.768651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.600300</td>\n",
       "      <td>2.769633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.624500</td>\n",
       "      <td>2.769070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.611200</td>\n",
       "      <td>2.769979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>2.769802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.627400</td>\n",
       "      <td>2.769064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.617200</td>\n",
       "      <td>2.768997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.617400</td>\n",
       "      <td>2.768608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.613000</td>\n",
       "      <td>2.769326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25144<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_023345-na5d8ndn\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_023345-na5d8ndn\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>2.76933</td></tr><tr><td>eval/runtime</td><td>22.5579</td></tr><tr><td>eval/samples_per_second</td><td>220.632</td></tr><tr><td>eval/steps_per_second</td><td>13.831</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>16413</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>2.613</td></tr><tr><td>train/total_flos</td><td>8574384177414144.0</td></tr><tr><td>train/train_loss</td><td>2.66716</td></tr><tr><td>train/train_runtime</td><td>3087.8868</td></tr><tr><td>train/train_samples_per_second</td><td>42.522</td></tr><tr><td>train/train_steps_per_second</td><td>5.315</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▃▂▃▁▃▄▅▅▅▄▆▅▄▆▇▅▄▅▅▅▅▆▅▇▅▄▅▆▅▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▇▆█▆▅▄▄▄▅▃▄▅▃▂▄▅▄▄▄▄▃▄▂▄▅▄▃▄▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▇▆█▆▅▄▄▄▅▃▄▅▃▂▅▅▄▄▄▄▃▄▂▄▅▄▃▄▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▂▂▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">royal-jazz-10</strong>: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/na5d8ndn\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/na5d8ndn</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lilac-donkey-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/ams99t7i\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/ams99t7i</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_032526-ams99t7i</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 54:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.826800</td>\n",
       "      <td>2.844819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.759000</td>\n",
       "      <td>2.846451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.762700</td>\n",
       "      <td>2.843015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.751900</td>\n",
       "      <td>2.844218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.738100</td>\n",
       "      <td>2.837416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.717000</td>\n",
       "      <td>2.839216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.713300</td>\n",
       "      <td>2.833132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.705500</td>\n",
       "      <td>2.831294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.690300</td>\n",
       "      <td>2.827171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.688000</td>\n",
       "      <td>2.824770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.678400</td>\n",
       "      <td>2.825119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.526400</td>\n",
       "      <td>2.833625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.541900</td>\n",
       "      <td>2.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.542000</td>\n",
       "      <td>2.831393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.539800</td>\n",
       "      <td>2.831067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.539400</td>\n",
       "      <td>2.827611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.530600</td>\n",
       "      <td>2.826571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>2.825503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.520700</td>\n",
       "      <td>2.823418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.518700</td>\n",
       "      <td>2.821159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.519400</td>\n",
       "      <td>2.819141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.508100</td>\n",
       "      <td>2.827852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.410400</td>\n",
       "      <td>2.836842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.407000</td>\n",
       "      <td>2.832689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.389500</td>\n",
       "      <td>2.834733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.410100</td>\n",
       "      <td>2.831932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.400700</td>\n",
       "      <td>2.832377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.403400</td>\n",
       "      <td>2.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.414300</td>\n",
       "      <td>2.831712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.404400</td>\n",
       "      <td>2.831118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.405200</td>\n",
       "      <td>2.831073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.401700</td>\n",
       "      <td>2.831066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5244<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_032526-ams99t7i\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_032526-ams99t7i\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>2.83107</td></tr><tr><td>eval/runtime</td><td>22.5915</td></tr><tr><td>eval/samples_per_second</td><td>220.304</td></tr><tr><td>eval/steps_per_second</td><td>13.81</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>16413</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4017</td></tr><tr><td>train/total_flos</td><td>8574384177414144.0</td></tr><tr><td>train/train_loss</td><td>2.55555</td></tr><tr><td>train/train_runtime</td><td>3254.7195</td></tr><tr><td>train/train_samples_per_second</td><td>40.343</td></tr><tr><td>train/train_steps_per_second</td><td>5.043</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>██▇▇▆▆▅▄▃▂▃▅▄▄▄▃▃▃▂▂▁▃▆▄▅▄▄▄▄▄▄▄</td></tr><tr><td>eval/runtime</td><td>▇█▃▂▁▂▁▁▂▁▁▂▁▂▂▃▄▄▅▄▆▆▆▅▅▅▆▅▅▄▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▂▁▆▇█▇██▇██▇█▇▇▆▅▅▄▅▃▃▃▄▄▄▃▄▄▅▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▂▁▆▇▇▇▇▇▇██▇█▇▇▆▅▅▄▅▃▃▃▄▄▄▃▄▄▅▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▇▇▆▆▆▆▆▆▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">lilac-donkey-11</strong>: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/ams99t7i\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/ams99t7i</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">amber-glade-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/b8gz3x77\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/b8gz3x77</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_041955-b8gz3x77</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 51:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.922000</td>\n",
       "      <td>2.837582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.763300</td>\n",
       "      <td>2.805496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.755300</td>\n",
       "      <td>2.795511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.746300</td>\n",
       "      <td>2.788532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.733100</td>\n",
       "      <td>2.784108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.717200</td>\n",
       "      <td>2.780734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.715300</td>\n",
       "      <td>2.778864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.710100</td>\n",
       "      <td>2.776872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.698000</td>\n",
       "      <td>2.774550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.698600</td>\n",
       "      <td>2.773267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.700500</td>\n",
       "      <td>2.771392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.660900</td>\n",
       "      <td>2.771598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.676500</td>\n",
       "      <td>2.770983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.675600</td>\n",
       "      <td>2.770476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.670300</td>\n",
       "      <td>2.769443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.670900</td>\n",
       "      <td>2.769507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.658800</td>\n",
       "      <td>2.768887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.666600</td>\n",
       "      <td>2.768210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.654300</td>\n",
       "      <td>2.767757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.656800</td>\n",
       "      <td>2.767274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.654400</td>\n",
       "      <td>2.767057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.658500</td>\n",
       "      <td>2.767110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.650700</td>\n",
       "      <td>2.767556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.642300</td>\n",
       "      <td>2.767028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.623300</td>\n",
       "      <td>2.767251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.647800</td>\n",
       "      <td>2.766994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.634300</td>\n",
       "      <td>2.767059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.641000</td>\n",
       "      <td>2.766938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.651300</td>\n",
       "      <td>2.766702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.641800</td>\n",
       "      <td>2.766588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.641900</td>\n",
       "      <td>2.766356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.637800</td>\n",
       "      <td>2.766402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24980<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_041955-b8gz3x77\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_041955-b8gz3x77\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>2.7664</td></tr><tr><td>eval/runtime</td><td>22.5419</td></tr><tr><td>eval/samples_per_second</td><td>220.789</td></tr><tr><td>eval/steps_per_second</td><td>13.841</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>16413</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.6378</td></tr><tr><td>train/total_flos</td><td>8574384177414144.0</td></tr><tr><td>train/train_loss</td><td>2.68266</td></tr><tr><td>train/train_runtime</td><td>3088.8856</td></tr><tr><td>train/train_samples_per_second</td><td>42.509</td></tr><tr><td>train/train_steps_per_second</td><td>5.314</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>█▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▇██████████████████████▁███████</td></tr><tr><td>eval/steps_per_second</td><td>▇▇██████████████████████▁███████</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">amber-glade-12</strong>: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/b8gz3x77\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/b8gz3x77</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
      "All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at C:\\Users\\zhuzi/.cache\\huggingface\\transformers\\29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
      "Model config GPTNeoConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">graceful-wildflower-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/2b7923vy\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/2b7923vy</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_051137-2b7923vy</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 43768\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16413\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16413' max='16413' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16413/16413 51:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.822200</td>\n",
       "      <td>2.828465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.742500</td>\n",
       "      <td>2.824556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.742600</td>\n",
       "      <td>2.819348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.730900</td>\n",
       "      <td>2.820631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.717300</td>\n",
       "      <td>2.811087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.696400</td>\n",
       "      <td>2.810147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.692800</td>\n",
       "      <td>2.811990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.685300</td>\n",
       "      <td>2.805316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.671600</td>\n",
       "      <td>2.803370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.670400</td>\n",
       "      <td>2.801223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.662200</td>\n",
       "      <td>2.807503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.520300</td>\n",
       "      <td>2.811570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.536400</td>\n",
       "      <td>2.810047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.536600</td>\n",
       "      <td>2.812808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.535200</td>\n",
       "      <td>2.810446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>2.810544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.527400</td>\n",
       "      <td>2.810366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.532800</td>\n",
       "      <td>2.808201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.521400</td>\n",
       "      <td>2.808004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.520100</td>\n",
       "      <td>2.806533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.522100</td>\n",
       "      <td>2.806678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.512400</td>\n",
       "      <td>2.816157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.422700</td>\n",
       "      <td>2.823946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>2.820920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.403700</td>\n",
       "      <td>2.823540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.425100</td>\n",
       "      <td>2.820348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.416200</td>\n",
       "      <td>2.821630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.419200</td>\n",
       "      <td>2.821477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.430100</td>\n",
       "      <td>2.820412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.420300</td>\n",
       "      <td>2.819529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.420500</td>\n",
       "      <td>2.818635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.416500</td>\n",
       "      <td>2.818391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-1000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-2000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-3000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-4000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-4000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-5000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-6000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-6000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-7000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-7000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-8000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-9000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-9000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-10000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-11000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-11000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-12000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-12000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-13000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-13000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-14000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-14000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-15000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4977\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models/gpt-neo-dialogs\\checkpoint-16000\n",
      "Configuration saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./models/gpt-neo-dialogs\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25312<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_051137-2b7923vy\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\zhuzi\\Documents\\Python Scripts\\gpt-fine-tuning\\wandb\\run-20210923_051137-2b7923vy\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>2.81839</td></tr><tr><td>eval/runtime</td><td>22.5675</td></tr><tr><td>eval/samples_per_second</td><td>220.538</td></tr><tr><td>eval/steps_per_second</td><td>13.825</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>16413</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4165</td></tr><tr><td>train/total_flos</td><td>8574384177414144.0</td></tr><tr><td>train/train_loss</td><td>2.55383</td></tr><tr><td>train/train_runtime</td><td>3091.4822</td></tr><tr><td>train/train_samples_per_second</td><td>42.473</td></tr><tr><td>train/train_steps_per_second</td><td>5.309</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>█▇▆▆▄▃▄▂▂▁▃▄▃▄▃▃▃▃▃▂▂▅▇▆▇▆▆▆▆▆▅▅</td></tr><tr><td>eval/runtime</td><td>▄█▄▁▃▄▄▄▃▆▄▅▄▄▄▆▄▅▅▆▅▄▅▅▅▅▄▄▅▅▅▅</td></tr><tr><td>eval/samples_per_second</td><td>▅▁▅█▆▅▅▅▅▃▅▄▅▅▅▃▅▄▄▃▄▅▄▄▄▄▅▅▄▄▄▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▁▅█▆▅▅▅▆▃▆▄▆▅▅▃▅▄▄▃▄▅▄▄▄▄▅▅▄▄▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▆▆▅▅▅▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">graceful-wildflower-13</strong>: <a href=\"https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/2b7923vy\" target=\"_blank\">https://wandb.ai/ziyizhu/gpt-neo-dialogs/runs/2b7923vy</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparams = [\n",
    "    (5e-5, 'linear', True), \n",
    "    (1e-5, 'cosine', False),\n",
    "    (5e-6, 'constant', False),\n",
    "    (5e-5, 'cosine', True),\n",
    "    (5e-6, 'linear', False),\n",
    "    (5e-5, 'linear', False),\n",
    "]\n",
    "\n",
    "for (learning_rate, lr_scheduler_type, adafactor) in hyperparams:\n",
    "    model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    wandb.init(project='gpt-neo-dialogs', entity='ziyizhu')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_path,         # The output directory\n",
    "        overwrite_output_dir=True,     # overwrite the content of the output directory\n",
    "        num_train_epochs=3,            # number of training epochs\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        adafactor=adafactor,\n",
    "        per_device_train_batch_size=8, # batch size for training\n",
    "        per_device_eval_batch_size=16, # batch size for evaluation\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,                # Number of update steps between two evaluations.\n",
    "        save_steps=1000,               # after # steps model is saved\n",
    "        warmup_steps=500,              # number of warmup steps for learning rate scheduler\n",
    "        report_to=\"wandb\",             # enable logging to W&B\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['test'],\n",
    "    )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    wandb.config.test_size = 0.1\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36772c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
